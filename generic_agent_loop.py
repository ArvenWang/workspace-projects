#!/usr/bin/env python3
"""
GenericAgent å¾ªç¯æ‰§è¡Œå™¨
å°è£…å¤æ—¦å¤§å­¦çš„ agent_runner_loop
"""

import json
import time
from dataclasses import dataclass
from typing import Any, Optional, List, Dict
from enum import Enum


class StepOutcome:
    """å•æ­¥æ‰§è¡Œç»“æœ"""
    def __init__(self, data: Any, next_prompt: str = None, should_exit: bool = False):
        self.data = data
        self.next_prompt = next_prompt or ""
        self.should_exit = should_exit


class GenericAgentLoop:
    """
    GenericAgent å¾ªç¯æ‰§è¡Œå™¨
    
    ç”¨æ³•:
        agent = GenericAgentLoop()
        result = agent.run("å¸®æˆ‘æŸ¥ä¸€ä¸‹å¤©æ°”")
    """
    
    def __init__(self, max_turns=40, verbose=True):
        self.max_turns = max_turns
        self.verbose = verbose
        self.messages = []
        self.turn = 0
        self.working_checkpoint = ""
        self.tools_schema = None
        self.system_prompt = self._get_default_system_prompt()
        
    def _get_default_system_prompt(self):
        """é»˜è®¤ç³»ç»Ÿæç¤ºè¯"""
        return """# Role: ç‰©ç†çº§å…¨èƒ½æ‰§è¡Œè€…
ä½ æ‹¥æœ‰æ–‡ä»¶è¯»å†™ã€è„šæœ¬æ‰§è¡Œã€ç”¨æˆ·æµè§ˆå™¨JSæ³¨å…¥ã€ç³»ç»Ÿçº§å¹²é¢„çš„ç‰©ç†æ“ä½œæƒé™ã€‚ç¦æ­¢æ¨è¯¿"æ— æ³•æ“ä½œ"â€”â€”ä¸ç©ºæƒ³ï¼Œç”¨å·¥å…·æ¢æµ‹ã€‚

## è¡ŒåŠ¨åŸåˆ™
è°ƒç”¨å·¥å…·å‰åœ¨ <thinking> å†…æ¨æ¼”ï¼šå½“å‰é˜¶æ®µã€ä¸Šæ­¥ç»“æœæ˜¯å¦ç¬¦åˆé¢„æœŸã€ä¸‹æ­¥ç­–ç•¥ã€‚
- æ¢æµ‹ä¼˜å…ˆï¼šå¤±è´¥æ—¶å…ˆå……åˆ†è·å–ä¿¡æ¯ï¼ˆæ—¥å¿—/çŠ¶æ€/ä¸Šä¸‹æ–‡ï¼‰ï¼Œå…³é”®ä¿¡æ¯å­˜å…¥å·¥ä½œè®°å¿†ï¼Œå†å†³å®šé‡è¯•æˆ–æ¢æ–¹æ¡ˆã€‚ä¸å¯é€†æ“ä½œå…ˆè¯¢é—®ç”¨æˆ·ã€‚
- å¤±è´¥å‡çº§ï¼š1æ¬¡â†’è¯»é”™è¯¯ç†è§£åŸå› ï¼Œ2æ¬¡â†’æ¢æµ‹ç¯å¢ƒçŠ¶æ€ï¼Œ3æ¬¡â†’æ·±åº¦åˆ†æåæ¢æ–¹æ¡ˆæˆ–é—®ç”¨æˆ·ã€‚ç¦æ­¢æ— æ–°ä¿¡æ¯çš„é‡å¤æ“ä½œã€‚

## å¯ç”¨å·¥å…·
- code_run: æ‰§è¡Œ Python/Bash ä»£ç 
- file_read: è¯»å–æ–‡ä»¶
- file_patch: å±€éƒ¨ä¿®æ”¹æ–‡ä»¶
- file_write: æ–°å»º/è¦†ç›–æ–‡ä»¶
- web_scan: è·å–é¡µé¢å†…å®¹
- web_execute_js: æ‰§è¡Œ JavaScript
- ask_user: å‘ç”¨æˆ·æé—®
- update_working_checkpoint: æ›´æ–°å·¥ä½œä¾¿ç­¾
- start_long_term_update: å‡†å¤‡é•¿æœŸè®°å¿†
"""
    
    def set_system_prompt(self, prompt: str):
        """è®¾ç½®ç³»ç»Ÿæç¤ºè¯"""
        self.system_prompt = prompt
    
    def set_tools_schema(self, schema: List[Dict]):
        """è®¾ç½®å·¥å…· schema"""
        self.tools_schema = schema
    
    def run(self, user_input: str, llm_client=None) -> Dict:
        """
        è¿è¡Œ Agent å¾ªç¯
        
        Args:
            user_input: ç”¨æˆ·è¾“å…¥
            llm_client: LLM å®¢æˆ·ç«¯ï¼ˆéœ€å®ç° chat æ–¹æ³•ï¼‰
            
        Returns:
            {'result': 'SUCCESS/EXITED/MAX_TURNS', 'data': ...}
        """
        self.messages = [
            {"role": "system", "content": self.system_prompt},
            {"role": "user", "content": user_input}
        ]
        self.turn = 0
        
        # å¦‚æœæ²¡æœ‰ LLM å®¢æˆ·ç«¯ï¼Œè¿”å›æç¤º
        if llm_client is None:
            return {
                "status": "error",
                "msg": "éœ€è¦é…ç½® LLM å®¢æˆ·ç«¯",
                "suggestion": "è¯·è®¾ç½® llm_client å‚æ•°"
            }
        
        for turn in range(self.max_turns):
            self.turn = turn + 1
            
            if self.verbose:
                print(f"\n{'='*50}")
                print(f"Turn {self.turn}/{self.max_turns}")
                print(f"{'='*50}")
            
            # è°ƒç”¨ LLM
            try:
                response = llm_client.chat(
                    messages=self.messages,
                    tools=self.tools_schema
                )
            except Exception as e:
                return {"status": "error", "msg": f"LLM è°ƒç”¨å¤±è´¥: {e}"}
            
            # è§£æå·¥å…·è°ƒç”¨
            if not response.get('tool_calls'):
                tool_name = 'no_tool'
                args = {}
            else:
                tool_call = response['tool_calls'][0]
                tool_name = tool_call.get('function', {}).get('name', '')
                args = json.loads(tool_call.get('function', {}).get('arguments', '{}'))
            
            if self.verbose:
                print(f"ğŸ› ï¸ å·¥å…·: {tool_name}")
                print(f"ğŸ“¥ å‚æ•°: {json.dumps(args, ensure_ascii=False)[:200]}...")
            
            # æ‰§è¡Œå·¥å…·
            from generic_agent_tools import execute_tool, WorkingCheckpoint
            
            tool_result = execute_tool(tool_name, **args)
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦ç”¨æˆ·å¹²é¢„
            if isinstance(tool_result, dict) and tool_result.get('status') == 'INTERRUPT':
                intent = tool_result.get('intent')
                
                if intent == 'HUMAN_INTERVENTION':
                    # éœ€è¦ç”¨æˆ·ç¡®è®¤
                    return {
                        "status": "NEED_USER_INPUT",
                        "question": tool_result['data']['question'],
                        "candidates": tool_result['data'].get('candidates', []),
                        "turn": self.turn
                    }
                elif intent == 'LONG_TERM_MEMORY_UPDATE':
                    # é•¿æœŸè®°å¿†æ›´æ–°
                    return {
                        "status": "MEMORY_UPDATE",
                        "message": tool_result['data']['message'],
                        "turn": self.turn
                    }
            
            if self.verbose:
                print(f"ğŸ“¤ ç»“æœ: {str(tool_result)[:200]}...")
            
            # æ„å»ºä¸‹ä¸€è½® prompt
            result_str = json.dumps(tool_result, ensure_ascii=False, default=str)[:4000]
            next_prompt = f"<tool_result>\n{result_str}\n</tool_result>\n\n"
            
            # å¦‚æœæœ‰å·¥ä½œä¾¿ç­¾ï¼Œæ³¨å…¥
            checkpoint = WorkingCheckpoint.get()
            if checkpoint:
                next_prompt += f"[å·¥ä½œä¾¿ç­¾]\n{checkpoint}\n\n"
            
            # æ·»åŠ ç”¨æˆ·çš„åç»­è¾“å…¥ï¼ˆå¦‚æœæœ‰ï¼‰
            self.messages = [{"role": "user", "content": next_prompt}]
            
            # æ£€æŸ¥é€€å‡ºæ¡ä»¶
            if tool_name == 'no_tool':
                return {"status": "SUCCESS", "data": response.get('content', ''), "turns": self.turn}
        
        return {"status": "MAX_TURNS_EXCEEDED", "turns": self.max_turns}
    
    def continue_with_input(self, user_response: str, llm_client) -> Dict:
        """ç»§ç»­å¤„ç†ç”¨æˆ·è¾“å…¥åçš„æƒ…å†µ"""
        self.messages.append({"role": "user", "content": user_response})
        
        for turn in range(self.max_turns - self.turn):
            self.turn += 1
            
            if self.verbose:
                print(f"\nTurn {self.turn}/{self.max_turns} (ç»§ç»­)")
            
            # è°ƒç”¨ LLM
            try:
                response = llm_client.chat(
                    messages=self.messages,
                    tools=self.tools_schema
                )
            except Exception as e:
                return {"status": "error", "msg": f"LLM è°ƒç”¨å¤±è´¥: {e}"}
            
            # è§£æå·¥å…·è°ƒç”¨
            if not response.get('tool_calls'):
                return {"status": "SUCCESS", "data": response.get('content', ''), "turns": self.turn}
            
            tool_call = response['tool_calls'][0]
            tool_name = tool_call.get('function', {}).get('name', '')
            args = json.loads(tool_call.get('function', {}).get('arguments', '{}'))
            
            # æ‰§è¡Œå·¥å…·
            from generic_agent_tools import execute_tool
            tool_result = execute_tool(tool_name, **args)
            
            # æ£€æŸ¥é€€å‡º
            if tool_name == 'no_tool':
                return {"status": "SUCCESS", "data": response.get('content', ''), "turns": self.turn}
            
            # ç»§ç»­å¾ªç¯
            result_str = json.dumps(tool_result, ensure_ascii=False, default=str)[:4000]
            next_prompt = f"<tool_result>\n{result_str}\n</tool_result>\n\n"
            self.messages = [{"role": "user", "content": next_prompt}]
        
        return {"status": "MAX_TURNS_EXCEEDED"}


# ========== æ¨¡æ‹Ÿ LLM å®¢æˆ·ç«¯ï¼ˆç”¨äºæµ‹è¯•ï¼‰==========
class MockLLMClient:
    """æ¨¡æ‹Ÿ LLM å®¢æˆ·ç«¯ - ç”¨äºæµ‹è¯•"""
    
    def __init__(self, responses=None):
        self.responses = responses or []
        self.call_count = 0
        self.history = []
    
    def chat(self, messages, tools=None):
        self.call_count += 1
        
        # è®°å½•å†å²
        last_msg = messages[-1]['content'] if messages else ''
        self.history.append(last_msg)
        
        # è¿”å›æ¨¡æ‹Ÿå“åº”
        if self.call_count <= len(self.responses):
            return self.responses[self.call_count - 1]
        
        # é»˜è®¤è¿”å›å®Œæˆ
        return {
            "content": "ä»»åŠ¡å·²å®Œæˆ",
            "tool_calls": []
        }


# ========== æµ‹è¯• ==========
if __name__ == '__main__':
    print("=== æµ‹è¯• GenericAgentLoop ===")
    
    # åˆ›å»ºæ¨¡æ‹Ÿå“åº”ï¼šç¬¬ä¸€æ¬¡è°ƒç”¨ file_readï¼Œç¬¬äºŒæ¬¡ç»“æŸ
    mock_responses = [
        {
            "content": "æˆ‘æ¥çœ‹çœ‹è¿™ä¸ªæ–‡ä»¶",
            "tool_calls": [
                {
                    "function": {
                        "name": "file_read",
                        "arguments": json.dumps({"path": "ga.py", "count": 10})
                    }
                }
            ]
        },
        {
            "content": "æ–‡ä»¶å†…å®¹å·²è¯»å–",
            "tool_calls": []
        }
    ]
    
    client = MockLLMClient(mock_responses)
    agent = GenericAgentLoop(verbose=True)
    
    result = agent.run("çœ‹çœ‹ ga.py çš„å†…å®¹", llm_client=client)
    print(f"\nç»“æœ: {result}")
