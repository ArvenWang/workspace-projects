#!/usr/bin/env python3
"""
æ¼«å‰§Top500æ•°æ®åˆ†æç³»ç»Ÿ
åŸºäºCSVæ•°æ®ç”Ÿæˆæ·±åº¦åˆ†ææŠ¥å‘Š
"""

import pandas as pd
import json
from pathlib import Path
from datetime import datetime
from collections import Counter


class ManjuAnalyzer:
    """æ¼«å‰§æ•°æ®åˆ†æå™¨"""
    
    def __init__(self):
        self.data_dir = Path(__file__).parent.parent.parent / "data"
        self.analysis_dir = Path(__file__).parent.parent.parent / "analysis"
        self.insights_dir = Path(__file__).parent.parent.parent / "insights"
        
        # åˆ›å»ºç›®å½•
        self.analysis_dir.mkdir(parents=True, exist_ok=True)
        self.insights_dir.mkdir(parents=True, exist_ok=True)
        
        self.df = None
        self.results = {}
    
    def load_data(self, csv_path=None):
        """åŠ è½½æ•°æ®"""
        if csv_path is None:
            # ä»researchç›®å½•åŠ è½½
            csv_path = Path(__file__).parent.parent.parent.parent / "ai_manju_data_2025.csv"
        
        print(f"ğŸ“Š åŠ è½½æ•°æ®: {csv_path}")
        self.df = pd.read_csv(csv_path, encoding='utf-8')
        print(f"âœ… åŠ è½½å®Œæˆï¼Œå…± {len(self.df)} æ¡è®°å½•")
        return self
    
    def analyze_genre_distribution(self):
        """åˆ†æé¢˜æåˆ†å¸ƒ"""
        print("\nğŸ­ åˆ†æé¢˜æåˆ†å¸ƒ...")
        
        genre_counts = self.df['é¢˜æç±»å‹'].value_counts()
        genre_stats = []
        
        for genre, count in genre_counts.items():
            subset = self.df[self.df['é¢˜æç±»å‹'] == genre]
            avg_views = subset['æ’­æ”¾é‡(äº¿)'].mean()
            avg_likes = subset['ç‚¹èµæ•°(ä¸‡)'].mean()
            
            genre_stats.append({
                'é¢˜æ': genre,
                'æ•°é‡': int(count),
                'å æ¯”': f"{count/len(self.df)*100:.1f}%",
                'å¹³å‡æ’­æ”¾é‡': f"{avg_views:.1f}äº¿",
                'å¹³å‡ç‚¹èµ': f"{avg_likes:.0f}ä¸‡"
            })
        
        self.results['é¢˜æåˆ†å¸ƒ'] = genre_stats
        
        print("\né¢˜æåˆ†å¸ƒTOP10:")
        for i, g in enumerate(genre_stats[:10], 1):
            print(f"  {i}. {g['é¢˜æ']}: {g['æ•°é‡']}éƒ¨ ({g['å æ¯”']}) - å‡æ’­{g['å¹³å‡æ’­æ”¾é‡']}")
        
        return genre_stats
    
    def analyze_platform_distribution(self):
        """åˆ†æå¹³å°åˆ†å¸ƒ"""
        print("\nğŸ“± åˆ†æå¹³å°åˆ†å¸ƒ...")
        
        platform_counts = self.df['å¹³å°'].value_counts()
        platform_stats = []
        
        for platform, count in platform_counts.items():
            subset = self.df[self.df['å¹³å°'] == platform]
            avg_views = subset['æ’­æ”¾é‡(äº¿)'].mean()
            total_views = subset['æ’­æ”¾é‡(äº¿)'].sum()
            
            platform_stats.append({
                'å¹³å°': platform,
                'æ•°é‡': int(count),
                'å æ¯”': f"{count/len(self.df)*100:.1f}%",
                'å¹³å‡æ’­æ”¾é‡': f"{avg_views:.1f}äº¿",
                'æ€»æ’­æ”¾é‡': f"{total_views:.1f}äº¿"
            })
        
        self.results['å¹³å°åˆ†å¸ƒ'] = platform_stats
        
        print("\nå¹³å°åˆ†å¸ƒ:")
        for p in platform_stats:
            print(f"  â€¢ {p['å¹³å°']}: {p['æ•°é‡']}éƒ¨ - å‡æ’­{p['å¹³å‡æ’­æ”¾é‡']} - æ€»è®¡{p['æ€»æ’­æ”¾é‡']}")
        
        return platform_stats
    
    def analyze_plot_patterns(self):
        """åˆ†æå‰§æƒ…å¥—è·¯"""
        print("\nğŸ“– åˆ†æå‰§æƒ…å¥—è·¯...")
        
        # æå–æ‰€æœ‰å‰§æƒ…å¥—è·¯
        all_patterns = []
        for patterns in self.df['å‰§æƒ…å¥—è·¯'].dropna():
            if '+' in str(patterns):
                all_patterns.extend([p.strip() for p in str(patterns).split('+')])
            else:
                all_patterns.append(str(patterns).strip())
        
        pattern_counts = Counter(all_patterns)
        pattern_stats = []
        
        for pattern, count in pattern_counts.most_common(20):
            pattern_stats.append({
                'å¥—è·¯': pattern,
                'å‡ºç°æ¬¡æ•°': count,
                'å æ¯”': f"{count/len(self.df)*100:.1f}%"
            })
        
        self.results['å‰§æƒ…å¥—è·¯TOP20'] = pattern_stats
        
        print("\nçƒ­é—¨å‰§æƒ…å¥—è·¯TOP10:")
        for i, p in enumerate(pattern_stats[:10], 1):
            print(f"  {i}. {p['å¥—è·¯']}: {p['å‡ºç°æ¬¡æ•°']}æ¬¡ ({p['å æ¯”']})")
        
        return pattern_stats
    
    def analyze_top_performers(self):
        """åˆ†æå¤´éƒ¨çˆ†æ¬¾"""
        print("\nğŸ† åˆ†æå¤´éƒ¨çˆ†æ¬¾...")
        
        # æŒ‰æ’­æ”¾é‡æ’åº
        top_views = self.df.nlargest(20, 'æ’­æ”¾é‡(äº¿)')
        top_list = []
        
        for _, row in top_views.iterrows():
            top_list.append({
                'æ’å': len(top_list) + 1,
                'å‰§å': row['æ¼«å‰§åç§°'],
                'å¹³å°': row['å¹³å°'],
                'æ’­æ”¾é‡': f"{row['æ’­æ”¾é‡(äº¿)']}äº¿",
                'ç‚¹èµ': f"{row['ç‚¹èµæ•°(ä¸‡)']}ä¸‡",
                'é¢˜æ': row['é¢˜æç±»å‹'],
                'å¥—è·¯': row['å‰§æƒ…å¥—è·¯']
            })
        
        self.results['æ’­æ”¾é‡TOP20'] = top_list
        
        print("\næ’­æ”¾é‡TOP10:")
        for t in top_list[:10]:
            print(f"  {t['æ’å']}. ã€Š{t['å‰§å']}ã€‹- {t['æ’­æ”¾é‡']} - {t['é¢˜æ']}")
        
        return top_list
    
    def analyze_target_audience(self):
        """åˆ†æç›®æ ‡å—ä¼—"""
        print("\nğŸ‘¥ åˆ†æç›®æ ‡å—ä¼—...")
        
        audience_counts = self.df['ç›®æ ‡å—ä¼—'].value_counts()
        audience_stats = []
        
        for audience, count in audience_counts.head(15).items():
            subset = self.df[self.df['ç›®æ ‡å—ä¼—'] == audience]
            avg_views = subset['æ’­æ”¾é‡(äº¿)'].mean()
            
            audience_stats.append({
                'å—ä¼—ç¾¤ä½“': audience,
                'æ•°é‡': int(count),
                'å æ¯”': f"{count/len(self.df)*100:.1f}%",
                'å¹³å‡æ’­æ”¾é‡': f"{avg_views:.1f}äº¿"
            })
        
        self.results['ç›®æ ‡å—ä¼—åˆ†æ'] = audience_stats
        
        print("\nä¸»è¦å—ä¼—ç¾¤ä½“:")
        for a in audience_stats[:10]:
            print(f"  â€¢ {a['å—ä¼—ç¾¤ä½“']}: {a['æ•°é‡']}éƒ¨ ({a['å æ¯”']}) - å‡æ’­{a['å¹³å‡æ’­æ”¾é‡']}")
        
        return audience_stats
    
    def analyze_episode_patterns(self):
        """åˆ†æé›†æ•°/æ—¶é•¿æ¨¡å¼"""
        print("\nâ±ï¸ åˆ†æé›†æ•°/æ—¶é•¿æ¨¡å¼...")
        
        avg_episodes = self.df['é›†æ•°'].mean()
        avg_duration = self.df['å•é›†æ—¶é•¿(åˆ†é’Ÿ)'].mean()
        
        # é›†æ•°åˆ†å¸ƒ
        episode_ranges = pd.cut(self.df['é›†æ•°'], 
                               bins=[0, 40, 60, 80, 100, 200], 
                               labels=['<40é›†', '40-60é›†', '60-80é›†', '80-100é›†', '>100é›†'])
        episode_dist = episode_ranges.value_counts().to_dict()
        
        # æ—¶é•¿åˆ†å¸ƒ
        duration_ranges = pd.cut(self.df['å•é›†æ—¶é•¿(åˆ†é’Ÿ)'], 
                                bins=[0, 1.5, 2, 2.5, 3, 10], 
                                labels=['<1.5åˆ†', '1.5-2åˆ†', '2-2.5åˆ†', '2.5-3åˆ†', '>3åˆ†'])
        duration_dist = duration_ranges.value_counts().to_dict()
        
        stats = {
            'å¹³å‡é›†æ•°': f"{avg_episodes:.1f}é›†",
            'å¹³å‡æ—¶é•¿': f"{avg_duration:.1f}åˆ†é’Ÿ",
            'é›†æ•°åˆ†å¸ƒ': {str(k): int(v) for k, v in episode_dist.items()},
            'æ—¶é•¿åˆ†å¸ƒ': {str(k): int(v) for k, v in duration_dist.items()}
        }
        
        self.results['é›†æ•°æ—¶é•¿åˆ†æ'] = stats
        
        print(f"\nå¹³å‡é›†æ•°: {stats['å¹³å‡é›†æ•°']}")
        print(f"å¹³å‡æ—¶é•¿: {stats['å¹³å‡æ—¶é•¿']}")
        print(f"\né›†æ•°åˆ†å¸ƒ:")
        for k, v in episode_dist.items():
            print(f"  â€¢ {k}: {v}éƒ¨")
        
        return stats
    
    def analyze_production_methods(self):
        """åˆ†æåˆ¶ä½œæ–¹å¼"""
        print("\nğŸ¬ åˆ†æåˆ¶ä½œæ–¹å¼...")
        
        method_counts = self.df['åˆ¶ä½œæ–¹å¼'].value_counts()
        method_stats = []
        
        for method, count in method_counts.items():
            subset = self.df[self.df['åˆ¶ä½œæ–¹å¼'] == method]
            avg_views = subset['æ’­æ”¾é‡(äº¿)'].mean()
            
            method_stats.append({
                'åˆ¶ä½œæ–¹å¼': method,
                'æ•°é‡': int(count),
                'å æ¯”': f"{count/len(self.df)*100:.1f}%",
                'å¹³å‡æ’­æ”¾é‡': f"{avg_views:.1f}äº¿"
            })
        
        self.results['åˆ¶ä½œæ–¹å¼åˆ†æ'] = method_stats
        
        print("\nåˆ¶ä½œæ–¹å¼åˆ†å¸ƒ:")
        for m in method_stats:
            print(f"  â€¢ {m['åˆ¶ä½œæ–¹å¼']}: {m['æ•°é‡']}éƒ¨ ({m['å æ¯”']}) - å‡æ’­{m['å¹³å‡æ’­æ”¾é‡']}")
        
        return method_stats
    
    def generate_insights(self):
        """ç”Ÿæˆæ ¸å¿ƒæ´å¯Ÿ"""
        print("\nğŸ’¡ ç”Ÿæˆæ ¸å¿ƒæ´å¯Ÿ...")
        
        insights = {
            'æ ¸å¿ƒå‘ç°': [
                f"æ ·æœ¬æ€»æ•°ï¼šå…±åˆ†æ{len(self.df)}éƒ¨çƒ­é—¨AIæ¼«å‰§",
                f"æ€»æ’­æ”¾é‡ï¼š{self.df['æ’­æ”¾é‡(äº¿)'].sum():.1f}äº¿æ¬¡",
                f"æ€»ç‚¹èµæ•°ï¼š{self.df['ç‚¹èµæ•°(ä¸‡)'].sum():.0f}ä¸‡æ¬¡",
                f"å¹³å‡æ’­æ”¾é‡ï¼š{self.df['æ’­æ”¾é‡(äº¿)'].mean():.1f}äº¿æ¬¡/éƒ¨",
                f"çˆ†æ¬¾ç‡ï¼ˆ>10äº¿æ’­æ”¾ï¼‰ï¼š{(self.df['æ’­æ”¾é‡(äº¿)'] > 10).sum()}éƒ¨ ({(self.df['æ’­æ”¾é‡(äº¿)'] > 10).sum()/len(self.df)*100:.1f}%)"
            ],
            'æˆåŠŸè¦ç´ ': [
                "1. é¢˜æé€‰æ‹©ï¼šä¿®ä»™ç„å¹»å’Œç”œå® æ‹çˆ±å æ®ä¸»å¯¼åœ°ä½ï¼Œåˆè®¡å æ¯”è¶…è¿‡40%",
                "2. å‰§æƒ…å¥—è·¯ï¼šé‡ç”Ÿé€†è¢­ã€åºŸæŸ´é€†è¢­ã€å…ˆå©šåçˆ±æ˜¯ä¸‰å¤§é»„é‡‘å¥—è·¯",
                "3. å¹³å°ç­–ç•¥ï¼šæŠ–éŸ³æµé‡æœ€å¤§ï¼Œå¿«æ‰‹ä¸‹æ²‰å¸‚åœºæ•ˆæœå¥½ï¼ŒBç«™é€‚åˆç²¾å“å†…å®¹",
                "4. åˆ¶ä½œæ–¹å¼ï¼šçº¯AIç”Ÿæˆä¸ºä¸»æµï¼ŒAI+äººå·¥ç²¾ä¿®èƒ½è·å¾—æ›´é«˜æ’­æ”¾é‡",
                "5. é›†æ•°æ§åˆ¶ï¼š80-100é›†æ˜¯é»„é‡‘é›†æ•°ï¼Œå•é›†2-3åˆ†é’Ÿæœ€ä½³"
            ],
            'åˆ›ä½œå»ºè®®': [
                "1. ä¼˜å…ˆé€‰æ‹©ï¼šéƒ½å¸‚é‡ç”Ÿã€ä¿®ä»™ç„å¹»ã€ç”œå® æ‹çˆ±ä¸‰å¤§çƒ­é—¨é¢˜æ",
                "2. å‰§æƒ…è®¾è®¡ï¼šå‰3ç§’å¿…é¡»æŠ“ä½è§‚ä¼—ï¼Œæ¯é›†ç»“å°¾ç•™é’©å­",
                "3. äººè®¾æ‰“é€ ï¼šä¸»è§’è¦æœ‰æ˜ç¡®ç›®æ ‡ï¼Œåæ´¾è¦æœ‰è¶³å¤Ÿå‹è¿«æ„Ÿ",
                "4. æƒ…ç»ªèŠ‚å¥ï¼šçˆ½ç‚¹è¦å¯†é›†ï¼Œè™ç‚¹è¦é€‚åº¦ï¼Œåè½¬è¦å‡ºäººæ„æ–™",
                "5. æ›´æ–°ç­–ç•¥ï¼šæ—¥æ›´æˆ–éš”æ—¥æ›´ï¼Œä¿æŒç”¨æˆ·ç²˜æ€§"
            ]
        }
        
        self.results['æ ¸å¿ƒæ´å¯Ÿ'] = insights
        
        print("\n" + "="*60)
        print("ğŸ“Š æ ¸å¿ƒå‘ç°")
        print("="*60)
        for finding in insights['æ ¸å¿ƒå‘ç°']:
            print(f"  âœ“ {finding}")
        
        return insights
    
    def save_analysis(self):
        """ä¿å­˜åˆ†æç»“æœ"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # ä¿å­˜JSON
        json_path = self.analysis_dir / f"manju_analysis_{timestamp}.json"
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, ensure_ascii=False, indent=2)
        print(f"\nğŸ’¾ åˆ†æç»“æœå·²ä¿å­˜: {json_path}")
        
        # ç”ŸæˆMarkdownæŠ¥å‘Š
        md_path = self.insights_dir / f"manju_insights_{timestamp}.md"
        self.generate_markdown_report(md_path)
        print(f"ğŸ’¾ æ´å¯ŸæŠ¥å‘Šå·²ä¿å­˜: {md_path}")
        
        return json_path, md_path
    
    def generate_markdown_report(self, filepath):
        """ç”ŸæˆMarkdownæ ¼å¼çš„æ´å¯ŸæŠ¥å‘Š"""
        md_content = f"""# AIæ¼«å‰§Top150æ•°æ®åˆ†ææŠ¥å‘Š

> ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M')}
> æ•°æ®æ¥æº: 2024-2025å¹´çƒ­é—¨AIæ¼«å‰§æ¦œå•
> æ ·æœ¬æ•°é‡: {len(self.df)}éƒ¨

---

## ğŸ“Š æ ¸å¿ƒæ•°æ®æ¦‚è§ˆ

"""
        
        # æ ¸å¿ƒå‘ç°
        if 'æ ¸å¿ƒæ´å¯Ÿ' in self.results:
            for finding in self.results['æ ¸å¿ƒæ´å¯Ÿ']['æ ¸å¿ƒå‘ç°']:
                md_content += f"- **{finding}**\n"
        
        md_content += "\n## ğŸ­ é¢˜æåˆ†å¸ƒTOP10\n\n"
        if 'é¢˜æåˆ†å¸ƒ' in self.results:
            md_content += "| æ’å | é¢˜æ | æ•°é‡ | å æ¯” | å¹³å‡æ’­æ”¾é‡ |\n"
            md_content += "|------|------|------|------|------------|\n"
            for i, g in enumerate(self.results['é¢˜æåˆ†å¸ƒ'][:10], 1):
                md_content += f"| {i} | {g['é¢˜æ']} | {g['æ•°é‡']} | {g['å æ¯”']} | {g['å¹³å‡æ’­æ”¾é‡']} |\n"
        
        md_content += "\n## ğŸ“± å¹³å°åˆ†å¸ƒ\n\n"
        if 'å¹³å°åˆ†å¸ƒ' in self.results:
            md_content += "| å¹³å° | æ•°é‡ | å æ¯” | å¹³å‡æ’­æ”¾é‡ | æ€»æ’­æ”¾é‡ |\n"
            md_content += "|------|------|------|------------|----------|\n"
            for p in self.results['å¹³å°åˆ†å¸ƒ']:
                md_content += f"| {p['å¹³å°']} | {p['æ•°é‡']} | {p['å æ¯”']} | {p['å¹³å‡æ’­æ”¾é‡']} | {p['æ€»æ’­æ”¾é‡']} |\n"
        
        md_content += "\n## ğŸ† æ’­æ”¾é‡TOP20\n\n"
        if 'æ’­æ”¾é‡TOP20' in self.results:
            md_content += "| æ’å | å‰§å | å¹³å° | æ’­æ”¾é‡ | ç‚¹èµ | é¢˜æ |\n"
            md_content += "|------|------|------|--------|------|------|\n"
            for t in self.results['æ’­æ”¾é‡TOP20']:
                md_content += f"| {t['æ’å']} | ã€Š{t['å‰§å']}ã€‹ | {t['å¹³å°']} | {t['æ’­æ”¾é‡']} | {t['ç‚¹èµ']} | {t['é¢˜æ']} |\n"
        
        md_content += "\n## ğŸ“– çƒ­é—¨å‰§æƒ…å¥—è·¯TOP10\n\n"
        if 'å‰§æƒ…å¥—è·¯TOP20' in self.results:
            for i, p in enumerate(self.results['å‰§æƒ…å¥—è·¯TOP20'][:10], 1):
                md_content += f"{i}. **{p['å¥—è·¯']}** - {p['å‡ºç°æ¬¡æ•°']}æ¬¡ ({p['å æ¯”']})\n"
        
        md_content += "\n## ğŸ’¡ æ ¸å¿ƒæ´å¯Ÿ\n\n"
        if 'æ ¸å¿ƒæ´å¯Ÿ' in self.results:
            md_content += "### æˆåŠŸè¦ç´ \n\n"
            for item in self.results['æ ¸å¿ƒæ´å¯Ÿ']['æˆåŠŸè¦ç´ ']:
                md_content += f"- {item}\n"
            
            md_content += "\n### åˆ›ä½œå»ºè®®\n\n"
            for item in self.results['æ ¸å¿ƒæ´å¯Ÿ']['åˆ›ä½œå»ºè®®']:
                md_content += f"- {item}\n"
        
        md_content += """

---

*æœ¬æŠ¥å‘Šç”±OpenClaw AIè‡ªåŠ¨ç”Ÿæˆ*
"""
        
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(md_content)
        
        return filepath
    
    def run_full_analysis(self):
        """è¿è¡Œå®Œæ•´åˆ†æ"""
        print("="*60)
        print("ğŸ¬ AIæ¼«å‰§æ•°æ®åˆ†æç³»ç»Ÿå¯åŠ¨")
        print("="*60)
        
        # åŠ è½½æ•°æ®
        self.load_data()
        
        # æ‰§è¡Œå„é¡¹åˆ†æ
        self.analyze_genre_distribution()
        self.analyze_platform_distribution()
        self.analyze_plot_patterns()
        self.analyze_top_performers()
        self.analyze_target_audience()
        self.analyze_episode_patterns()
        self.analyze_production_methods()
        self.generate_insights()
        
        # ä¿å­˜ç»“æœ
        self.save_analysis()
        
        print("\n" + "="*60)
        print("âœ… åˆ†æå®Œæˆï¼")
        print("="*60)
        
        return self.results


if __name__ == '__main__':
    analyzer = ManjuAnalyzer()
    analyzer.run_full_analysis()
