#!/usr/bin/env python3
"""
æ¼«å‰§æ•°æ®é‡‡é›† - ä½¿ç”¨çœŸå®Chromeï¼ˆå·²ç™»å½•çŠ¶æ€ï¼‰
é‡‡é›†å°çº¢ä¹¦ã€Bç«™ã€æŠ–éŸ³ã€å¿«æ‰‹ç­‰å¹³å°
"""

import asyncio
import json
import csv
from datetime import datetime
from pathlib import Path
from playwright.async_api import async_playwright


class RealChromeCollector:
    """ä½¿ç”¨çœŸå®Chromeçš„é‡‡é›†å™¨"""
    
    def __init__(self):
        self.data_dir = Path(__file__).parent.parent.parent / "data" / "collected"
        self.data_dir.mkdir(parents=True, exist_ok=True)
        self.results = []
    
    async def __aenter__(self):
        self.playwright = await async_playwright().start()
        
        # ä½¿ç”¨çœŸå®Chromeï¼ˆå·²ç™»å½•çŠ¶æ€ï¼‰
        print("ğŸŒ å¯åŠ¨çœŸå®Chromeæµè§ˆå™¨...")
        print("   å°†ä½¿ç”¨ä½ çš„ç™»å½•çŠ¶æ€è®¿é—®å„å¹³å°")
        
        self.browser = await self.playwright.chromium.launch(
            headless=False,  # å¯è§æ¨¡å¼ï¼Œæ–¹ä¾¿è§‚å¯Ÿ
            executable_path="/Applications/Google Chrome.app/Contents/MacOS/Google Chrome"
        )
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        await self.browser.close()
        await self.playwright.stop()
    
    async def collect_xiaohongshu(self):
        """é‡‡é›†å°çº¢ä¹¦AIæ¼«å‰§æ•°æ®"""
        print("\n" + "="*60)
        print("ğŸ“• é‡‡é›†å°çº¢ä¹¦ - AIæ¼«å‰§æœç´¢")
        print("="*60)
        
        context = await self.browser.new_context(
            viewport={'width': 1440, 'height': 900}
        )
        page = await context.new_page()
        
        try:
            # è®¿é—®å°çº¢ä¹¦æœç´¢
            print("ğŸ” æœç´¢å…³é”®è¯: AIæ¼«å‰§")
            await page.goto('https://www.xiaohongshu.com/search_result/?keyword=AI%E6%BC%AB%E5%89%A7', 
                          wait_until='networkidle', timeout=60000)
            
            # ç­‰å¾…é¡µé¢åŠ è½½
            await page.wait_for_timeout(5000)
            
            # æå–ç¬”è®°æ•°æ®
            print("ğŸ“Š æå–ç¬”è®°æ•°æ®...")
            
            notes = []
            # æ»šåŠ¨åŠ è½½æ›´å¤š
            for i in range(3):
                await page.evaluate('window.scrollTo(0, document.body.scrollHeight)')
                await page.wait_for_timeout(2000)
            
            # è·å–ç¬”è®°å¡ç‰‡
            cards = await page.query_selector_all('.note-item, .feeds-page .note, [class*="note"]')
            print(f"   æ‰¾åˆ° {len(cards)} ä¸ªç¬”è®°")
            
            for idx, card in enumerate(cards[:30]):  # é™åˆ¶å‰30æ¡
                try:
                    # æå–æ ‡é¢˜
                    title_el = await card.query_selector('.title, .note-title, h3, .content span')
                    title = await title_el.text_content() if title_el else ''
                    
                    # æå–ä½œè€…
                    author_el = await card.query_selector('.author, .user-name, .nickname')
                    author = await author_el.text_content() if author_el else ''
                    
                    # æå–ç‚¹èµ
                    like_el = await card.query_selector('.like-count, .count, .likes')
                    likes = await like_el.text_content() if like_el else ''
                    
                    if title.strip():
                        notes.append({
                            'rank': idx + 1,
                            'platform': 'å°çº¢ä¹¦',
                            'keyword': 'AIæ¼«å‰§',
                            'title': title.strip()[:100],
                            'author': author.strip()[:50] if author else '',
                            'likes': likes.strip() if likes else '',
                            'collected_at': datetime.now().isoformat()
                        })
                except Exception as e:
                    continue
            
            # æˆªå›¾
            screenshot_path = self.data_dir / f"xiaohongshu_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png"
            await page.screenshot(path=str(screenshot_path), full_page=True)
            
            result = {
                'platform': 'å°çº¢ä¹¦',
                'keyword': 'AIæ¼«å‰§',
                'url': page.url,
                'notes_count': len(notes),
                'notes': notes,
                'screenshot': str(screenshot_path),
                'collected_at': datetime.now().isoformat()
            }
            
            self.results.append(result)
            print(f"âœ… å°çº¢ä¹¦é‡‡é›†å®Œæˆ: {len(notes)} æ¡ç¬”è®°")
            
            # ä¿å­˜CSV
            if notes:
                csv_path = self.data_dir / 'xiaohongshu_ai_manju.csv'
                with open(csv_path, 'w', newline='', encoding='utf-8-sig') as f:
                    writer = csv.DictWriter(f, fieldnames=notes[0].keys())
                    writer.writeheader()
                    writer.writerows(notes)
                print(f"ğŸ’¾ CSVå·²ä¿å­˜: {csv_path}")
            
            return result
            
        except Exception as e:
            print(f"âŒ å°çº¢ä¹¦é‡‡é›†å¤±è´¥: {str(e)}")
            return {'platform': 'å°çº¢ä¹¦', 'error': str(e)}
        finally:
            await context.close()
    
    async def collect_bilibili(self):
        """é‡‡é›†Bç«™çŸ­å‰§æ•°æ®"""
        print("\n" + "="*60)
        print("ğŸ“º é‡‡é›†Bç«™ - çŸ­å‰§åˆ†åŒº")
        print("="*60)
        
        context = await self.browser.new_context(
            viewport={'width': 1440, 'height': 900}
        )
        page = await context.new_page()
        
        try:
            # è®¿é—®Bç«™çŸ­å‰§åˆ†åŒº
            print("ğŸŒ è®¿é—®Bç«™çŸ­å‰§åˆ†åŒº...")
            await page.goto('https://www.bilibili.com/v/channel/shortplay',
                          wait_until='networkidle', timeout=60000)
            
            await page.wait_for_timeout(5000)
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦éªŒè¯
            if 'éªŒè¯ç ' in await page.title() or await page.query_selector('.geetest'):
                print("âš ï¸  é‡åˆ°éªŒè¯ç ï¼Œè¯·æ‰‹åŠ¨å®ŒæˆéªŒè¯...")
                print("   ä½ æœ‰30ç§’æ—¶é—´å®ŒæˆéªŒè¯")
                await page.wait_for_timeout(30000)  # ç­‰å¾…30ç§’è®©ç”¨æˆ·æ‰‹åŠ¨éªŒè¯
            
            print("ğŸ“Š æå–è§†é¢‘æ•°æ®...")
            
            videos = []
            # æ»šåŠ¨åŠ è½½
            for i in range(3):
                await page.evaluate('window.scrollTo(0, document.body.scrollHeight)')
                await page.wait_for_timeout(2000)
            
            # è·å–è§†é¢‘å¡ç‰‡
            cards = await page.query_selector_all('.video-card, .bili-video-card, .video-list-item')
            print(f"   æ‰¾åˆ° {len(cards)} ä¸ªè§†é¢‘")
            
            for idx, card in enumerate(cards[:30]):
                try:
                    # æå–æ ‡é¢˜
                    title_el = await card.query_selector('h3, .title, a[title]')
                    title = await title_el.get_attribute('title') if title_el else ''
                    if not title:
                        title = await title_el.text_content() if title_el else ''
                    
                    # æå–æ’­æ”¾é‡
                    play_el = await card.query_selector('.play-text, .view, .play-count')
                    plays = await play_el.text_content() if play_el else ''
                    
                    # æå–UPä¸»
                    author_el = await card.query_selector('.up-name, .author, .name')
                    author = await author_el.text_content() if author_el else ''
                    
                    if title.strip():
                        videos.append({
                            'rank': idx + 1,
                            'platform': 'Bç«™',
                            'title': title.strip()[:100],
                            'author': author.strip()[:50] if author else '',
                            'plays': plays.strip() if plays else '',
                            'collected_at': datetime.now().isoformat()
                        })
                except Exception as e:
                    continue
            
            # æˆªå›¾
            screenshot_path = self.data_dir / f"bilibili_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png"
            await page.screenshot(path=str(screenshot_path), full_page=True)
            
            result = {
                'platform': 'Bç«™',
                'url': page.url,
                'videos_count': len(videos),
                'videos': videos,
                'screenshot': str(screenshot_path),
                'collected_at': datetime.now().isoformat()
            }
            
            self.results.append(result)
            print(f"âœ… Bç«™é‡‡é›†å®Œæˆ: {len(videos)} ä¸ªè§†é¢‘")
            
            # ä¿å­˜CSV
            if videos:
                csv_path = self.data_dir / 'bilibili_shortplay.csv'
                with open(csv_path, 'w', newline='', encoding='utf-8-sig') as f:
                    writer = csv.DictWriter(f, fieldnotes=videos[0].keys())
                    writer.writeheader()
                    writer.writerows(videos)
                print(f"ğŸ’¾ CSVå·²ä¿å­˜: {csv_path}")
            
            return result
            
        except Exception as e:
            print(f"âŒ Bç«™é‡‡é›†å¤±è´¥: {str(e)}")
            return {'platform': 'Bç«™', 'error': str(e)}
        finally:
            await context.close()
    
    async def collect_douyin(self):
        """é‡‡é›†æŠ–éŸ³çŸ­å‰§æ•°æ®"""
        print("\n" + "="*60)
        print("ğŸµ é‡‡é›†æŠ–éŸ³ - AIçŸ­å‰§")
        print("="*60)
        
        context = await self.browser.new_context(
            viewport={'width': 1440, 'height': 900}
        )
        page = await context.new_page()
        
        try:
            print("ğŸŒ è®¿é—®æŠ–éŸ³æœç´¢...")
            await page.goto('https://www.douyin.com/search/AI%E6%BC%AB%E5%89%A7',
                          wait_until='domcontentloaded', timeout=60000)
            
            await page.wait_for_timeout(8000)  # æŠ–éŸ³åŠ è½½è¾ƒæ…¢
            
            print("ğŸ“Š æå–è§†é¢‘æ•°æ®...")
            
            videos = []
            # æ»šåŠ¨åŠ è½½
            for i in range(3):
                await page.evaluate('window.scrollTo(0, document.body.scrollHeight)')
                await page.wait_for_timeout(3000)
            
            # è·å–è§†é¢‘å¡ç‰‡
            cards = await page.query_selector_all('[class*="card"], [class*="item"], .search-card')
            print(f"   æ‰¾åˆ° {len(cards)} ä¸ªè§†é¢‘")
            
            for idx, card in enumerate(cards[:30]):
                try:
                    # æå–æ ‡é¢˜
                    title_el = await card.query_selector('h3, .title, span[class*="title"], [class*="desc"]')
                    title = await title_el.text_content() if title_el else ''
                    
                    # æå–ç‚¹èµ
                    like_el = await card.query_selector('[class*="like"], [class*="count"], [class*="stats"]')
                    likes = await like_el.text_content() if like_el else ''
                    
                    if title.strip() and len(title.strip()) > 5:
                        videos.append({
                            'rank': idx + 1,
                            'platform': 'æŠ–éŸ³',
                            'title': title.strip()[:100],
                            'likes': likes.strip()[:30] if likes else '',
                            'collected_at': datetime.now().isoformat()
                        })
                except Exception as e:
                    continue
            
            # æˆªå›¾
            screenshot_path = self.data_dir / f"douyin_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png"
            await page.screenshot(path=str(screenshot_path), full_page=True)
            
            result = {
                'platform': 'æŠ–éŸ³',
                'url': page.url,
                'videos_count': len(videos),
                'videos': videos,
                'screenshot': str(screenshot_path),
                'collected_at': datetime.now().isoformat()
            }
            
            self.results.append(result)
            print(f"âœ… æŠ–éŸ³é‡‡é›†å®Œæˆ: {len(videos)} ä¸ªè§†é¢‘")
            
            # ä¿å­˜CSV
            if videos:
                csv_path = self.data_dir / 'douyin_ai_manju.csv'
                with open(csv_path, 'w', newline='', encoding='utf-8-sig') as f:
                    writer = csv.DictWriter(f, fieldnotes=videos[0].keys())
                    writer.writeheader()
                    writer.writerows(videos)
                print(f"ğŸ’¾ CSVå·²ä¿å­˜: {csv_path}")
            
            return result
            
        except Exception as e:
            print(f"âŒ æŠ–éŸ³é‡‡é›†å¤±è´¥: {str(e)}")
            return {'platform': 'æŠ–éŸ³', 'error': str(e)}
        finally:
            await context.close()
    
    async def collect_kuaishou(self):
        """é‡‡é›†å¿«æ‰‹çŸ­å‰§æ•°æ®"""
        print("\n" + "="*60)
        print("ğŸ“± é‡‡é›†å¿«æ‰‹ - çŸ­å‰§")
        print("="*60)
        
        context = await self.browser.new_context(
            viewport={'width': 1440, 'height': 900}
        )
        page = await context.new_page()
        
        try:
            print("ğŸŒ è®¿é—®å¿«æ‰‹æœç´¢...")
            await page.goto('https://www.kuaishou.com/search?searchKey=AI%E6%BC%AB%E5%89%A7',
                          wait_until='networkidle', timeout=60000)
            
            await page.wait_for_timeout(5000)
            
            print("ğŸ“Š æå–è§†é¢‘æ•°æ®...")
            
            videos = []
            # æ»šåŠ¨åŠ è½½
            for i in range(3):
                await page.evaluate('window.scrollTo(0, document.body.scrollHeight)')
                await page.wait_for_timeout(2000)
            
            # è·å–è§†é¢‘å¡ç‰‡
            cards = await page.query_selector_all('.video-item, .search-item, [class*="item"]')
            print(f"   æ‰¾åˆ° {len(cards)} ä¸ªè§†é¢‘")
            
            for idx, card in enumerate(cards[:30]):
                try:
                    # æå–æ ‡é¢˜
                    title_el = await card.query_selector('.title, h3, [class*="title"], [class*="desc"]')
                    title = await title_el.text_content() if title_el else ''
                    
                    # æå–ä½œè€…
                    author_el = await card.query_selector('.author, .user-name, [class*="user"]')
                    author = await author_el.text_content() if author_el else ''
                    
                    if title.strip() and len(title.strip()) > 5:
                        videos.append({
                            'rank': idx + 1,
                            'platform': 'å¿«æ‰‹',
                            'title': title.strip()[:100],
                            'author': author.strip()[:50] if author else '',
                            'collected_at': datetime.now().isoformat()
                        })
                except Exception as e:
                    continue
            
            # æˆªå›¾
            screenshot_path = self.data_dir / f"kuaishou_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png"
            await page.screenshot(path=str(screenshot_path), full_page=True)
            
            result = {
                'platform': 'å¿«æ‰‹',
                'url': page.url,
                'videos_count': len(videos),
                'videos': videos,
                'screenshot': str(screenshot_path),
                'collected_at': datetime.now().isoformat()
            }
            
            self.results.append(result)
            print(f"âœ… å¿«æ‰‹é‡‡é›†å®Œæˆ: {len(videos)} ä¸ªè§†é¢‘")
            
            # ä¿å­˜CSV
            if videos:
                csv_path = self.data_dir / 'kuaishou_ai_manju.csv'
                with open(csv_path, 'w', newline='', encoding='utf-8-sig') as f:
                    writer = csv.DictWriter(f, fieldnotes=videos[0].keys())
                    writer.writeheader()
                    writer.writerows(videos)
                print(f"ğŸ’¾ CSVå·²ä¿å­˜: {csv_path}")
            
            return result
            
        except Exception as e:
            print(f"âŒ å¿«æ‰‹é‡‡é›†å¤±è´¥: {str(e)}")
            return {'platform': 'å¿«æ‰‹', 'error': str(e)}
        finally:
            await context.close()
    
    def save_summary(self):
        """ä¿å­˜é‡‡é›†æ±‡æ€»"""
        summary_path = self.data_dir / f"collection_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(summary_path, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, ensure_ascii=False, indent=2)
        print(f"\nğŸ’¾ æ±‡æ€»æŠ¥å‘Šå·²ä¿å­˜: {summary_path}")
        return summary_path


async def main():
    """ä¸»ç¨‹åº"""
    print("ğŸ¬ æ¼«å‰§æ•°æ®é‡‡é›†ç³»ç»Ÿ - çœŸå®Chromeæ¨¡å¼")
    print("="*60)
    print("âš ï¸  å°†ä½¿ç”¨ä½ çš„Chromeæµè§ˆå™¨å’Œç™»å½•çŠ¶æ€")
    print("   è¯·å‹¿åœ¨é‡‡é›†è¿‡ç¨‹ä¸­æ“ä½œæµè§ˆå™¨")
    print("="*60)
    
    async with RealChromeCollector() as collector:
        # é‡‡é›†å„å¹³å°
        await collector.collect_xiaohongshu()
        await collector.collect_bilibili()
        await collector.collect_douyin()
        await collector.collect_kuaishou()
        
        # ä¿å­˜æ±‡æ€»
        collector.save_summary()
    
    print("\n" + "="*60)
    print("âœ… æ‰€æœ‰å¹³å°é‡‡é›†å®Œæˆï¼")
    print("="*60)
    print(f"\nğŸ“ æ•°æ®ä¿å­˜ä½ç½®: {collector.data_dir}")


if __name__ == '__main__':
    asyncio.run(main())
