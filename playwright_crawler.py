#!/usr/bin/env python3
"""
Playwright çˆ¬è™«è„šæœ¬ç¤ºä¾‹
ç”¨äºæ¼”ç¤ºç½‘é¡µæ•°æ®é‡‡é›†åŠŸèƒ½
"""

import asyncio
import json
from playwright.async_api import async_playwright
from datetime import datetime


class WebCrawler:
    """ç½‘é¡µçˆ¬è™«ç±»"""
    
    def __init__(self, headless=True):
        self.headless = headless
        self.results = []
    
    async def __aenter__(self):
        self.playwright = await async_playwright().start()
        self.browser = await self.playwright.chromium.launch(headless=self.headless)
        self.context = await self.browser.new_context(
            viewport={'width': 1920, 'height': 1080},
            user_agent='Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
        )
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        await self.context.close()
        await self.browser.close()
        await self.playwright.stop()
    
    async def crawl_page(self, url, wait_for=None, extract_selectors=None):
        """
        çˆ¬å–å•ä¸ªé¡µé¢
        
        Args:
            url: è¦çˆ¬å–çš„URL
            wait_for: ç­‰å¾…çš„å…ƒç´ é€‰æ‹©å™¨
            extract_selectors: è¦æå–æ•°æ®çš„CSSé€‰æ‹©å™¨å­—å…¸
        """
        page = await self.context.new_page()
        
        try:
            print(f"ğŸŒ æ­£åœ¨è®¿é—®: {url}")
            await page.goto(url, wait_until='networkidle', timeout=30000)
            
            # ç­‰å¾…ç‰¹å®šå…ƒç´ åŠ è½½
            if wait_for:
                await page.wait_for_selector(wait_for, timeout=10000)
            
            # æå–æ•°æ®
            data = {
                'url': url,
                'title': await page.title(),
                'timestamp': datetime.now().isoformat(),
            }
            
            # æ ¹æ®é€‰æ‹©å™¨æå–è‡ªå®šä¹‰æ•°æ®
            if extract_selectors:
                for key, selector in extract_selectors.items():
                    try:
                        elements = await page.query_selector_all(selector)
                        texts = []
                        for el in elements[:10]:  # é™åˆ¶æ•°é‡
                            text = await el.text_content()
                            if text:
                                texts.append(text.strip())
                        data[key] = texts
                    except Exception as e:
                        data[key] = f"æå–å¤±è´¥: {str(e)}"
            
            # æˆªå›¾ä¿å­˜
            screenshot_path = f"crawl_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png"
            await page.screenshot(path=screenshot_path, full_page=True)
            data['screenshot'] = screenshot_path
            
            self.results.append(data)
            print(f"âœ… æˆåŠŸçˆ¬å–: {data['title']}")
            return data
            
        except Exception as e:
            print(f"âŒ çˆ¬å–å¤±è´¥: {str(e)}")
            return {'url': url, 'error': str(e)}
        finally:
            await page.close()
    
    async def search_and_extract(self, keyword, search_engine='google'):
        """
        æœç´¢å…³é”®è¯å¹¶æå–ç»“æœ
        """
        if search_engine == 'google':
            url = f"https://www.google.com/search?q={keyword.replace(' ', '+')}"
            selectors = {
                'results': 'div[data-header-feature] h3, div.g h3',
                'descriptions': 'div[data-content-feature] span, div.g span'
            }
        elif search_engine == 'bing':
            url = f"https://www.bing.com/search?q={keyword.replace(' ', '+')}"
            selectors = {
                'results': 'h2 a',
                'descriptions': '.b_caption p'
            }
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æœç´¢å¼•æ“: {search_engine}")
        
        return await self.crawl_page(url, wait_for='body', extract_selectors=selectors)
    
    def save_results(self, filename='crawl_results.json'):
        """ä¿å­˜ç»“æœåˆ°JSONæ–‡ä»¶"""
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, ensure_ascii=False, indent=2)
        print(f"ğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {filename}")


async def demo():
    """æ¼”ç¤ºçˆ¬è™«åŠŸèƒ½"""
    print("ğŸš€ å¯åŠ¨ Playwright çˆ¬è™«æ¼”ç¤º\n")
    
    async with WebCrawler(headless=False) as crawler:
        # ç¤ºä¾‹1: çˆ¬å–ç¤ºä¾‹ç½‘ç«™
        print("=" * 50)
        print("ç¤ºä¾‹ 1: çˆ¬å– httpbin.org")
        print("=" * 50)
        
        result = await crawler.crawl_page(
            url='https://httpbin.org/html',
            extract_selectors={
                'headers': 'h1',
                'paragraphs': 'p'
            }
        )
        print(f"æå–çš„æ•°æ®: {json.dumps(result, ensure_ascii=False, indent=2)}\n")
        
        # ç¤ºä¾‹2: çˆ¬å– GitHub é¦–é¡µ
        print("=" * 50)
        print("ç¤ºä¾‹ 2: çˆ¬å– GitHub é¦–é¡µ")
        print("=" * 50)
        
        result = await crawler.crawl_page(
            url='https://github.com',
            wait_for='.application-main',
            extract_selectors={
                'headings': 'h1, h2, h3',
                'links': 'a[href^="/"]'
            }
        )
        print(f"æå–çš„æ•°æ®: {json.dumps(result, ensure_ascii=False, indent=2)}\n")
        
        # ä¿å­˜ç»“æœ
        crawler.save_results('demo_crawl_results.json')
    
    print("\nâœ… çˆ¬è™«æ¼”ç¤ºå®Œæˆï¼")


if __name__ == '__main__':
    asyncio.run(demo())
