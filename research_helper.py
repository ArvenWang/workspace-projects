#!/usr/bin/env python3
"""
æ•°æ®ç ”ç©¶åŠ©æ‰‹ - å®Œæ•´ç‰ˆ
åŠŸèƒ½ï¼š
1. ç½‘é¡µæ•°æ®é‡‡é›†
2. æ•°æ®åˆ†æ
3. æŠ¥å‘Šç”Ÿæˆ
4. å›¾è¡¨åˆ¶ä½œ

ä¾èµ–ï¼š
pip3 install requests beautifulsoup4 matplotlib pandas

è¿è¡Œï¼š
python3 research_helper.py collect <URL>
python3 research_helper.py analyze <æ–‡ä»¶>
python3 research_helper.py report <ä¸»é¢˜>
"""

import requests
import json
import os
import re
from datetime import datetime
from pathlib import Path
from collections import Counter

# é…ç½®
CONFIG = {
    'data_dir': os.path.expanduser('~/.research_helper'),
    'timeout': 30,
}

Path(CONFIG['data_dir']).mkdir(parents=True, exist_ok=True)


class ResearchHelper:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
        })
    
    def collect_url(self, url):
        """é‡‡é›†ç½‘é¡µæ•°æ®"""
        try:
            print(f"ğŸ” é‡‡é›†: {url}")
            
            resp = self.session.get(url, timeout=CONFIG['timeout'])
            resp.raise_for_status()
            
            # æå–åŸºæœ¬ä¿¡æ¯
            data = {
                'url': url,
                'status': resp.status_code,
                'title': self.extract_title(resp.text),
                'description': self.extract_description(resp.text),
                'links': self.extract_links(resp.text),
                'images': self.extract_images(resp.text),
                'text_length': len(resp.text),
                'collected_at': datetime.now().isoformat()
            }
            
            # ä¿å­˜
            self.save_data(url, data)
            
            print(f"âœ… é‡‡é›†æˆåŠŸ")
            print(f"   æ ‡é¢˜: {data['title']}")
            print(f"   æè¿°: {data['description'][:50]}...")
            print(f"   é“¾æ¥æ•°: {len(data['links'])}")
            print(f"   å›¾ç‰‡æ•°: {len(data['images'])}")
            
            return data
            
        except Exception as e:
            print(f"âŒ é‡‡é›†å¤±è´¥: {e}")
            return None
    
    def extract_title(self, html):
        """æå–æ ‡é¢˜"""
        match = re.search(r'<title>([^<]+)</title>', html, re.I)
        return match.group(1).strip() if match else ''
    
    def extract_description(self, html):
        """æå–æè¿°"""
        match = re.search(r'<meta[^>]+name=["\']description["\'][^>]+content=["\']([^"\']+)["\']', html, re.I)
        if not match:
            match = re.search(r'<meta[^>]+content=["\']([^"\']+)["\'][^>]+name=["\']description["\']', html, re.I)
        return match.group(1).strip() if match else ''
    
    def extract_links(self, html, limit=50):
        """æå–é“¾æ¥"""
        links = re.findall(r'href=["\'](https?://[^"\']+)["\']', html)
        # å»é‡
        links = list(set(links))[:limit]
        return links
    
    def extract_images(self, html, limit=20):
        """æå–å›¾ç‰‡"""
        images = re.findall(r'src=["\'](https?://[^"\']+\.(jpg|jpeg|png|gif|webp)[^"\']*)["\']', html, re.I)
        images = [img[0] for img in images]
        return list(set(images))[:limit]
    
    def save_data(self, url, data):
        """ä¿å­˜æ•°æ®"""
        filename = self.sanitize_filename(url) + '.json'
        filepath = os.path.join(CONFIG['data_dir'], filename)
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
    
    def sanitize_filename(self, url):
        """ç”Ÿæˆå®‰å…¨çš„æ–‡ä»¶å"""
        name = url.replace('https://', '').replace('http://', '')
        name = re.sub(r'[^\w\-]', '_', name)[:50]
        return name
    
    def list_collected(self):
        """åˆ—å‡ºå·²é‡‡é›†æ•°æ®"""
        files = list(Path(CONFIG['data_dir']).glob('*.json'))
        print(f"\nå·²é‡‡é›† {len(files)} ä¸ªé¡µé¢:")
        for f in files:
            print(f"  - {f.stem[:40]}")
    
    def analyze_text(self, text):
        """åˆ†ææ–‡æœ¬"""
        words = re.findall(r'[\w]+', text.lower())
        word_count = Counter(words)
        
        # è¿‡æ»¤åœç”¨è¯
        stopwords = {'the', 'a', 'an', 'is', 'are', 'was', 'were', 'be', 'been', 
                    'being', 'have', 'has', 'had', 'do', 'does', 'did', 'will',
                    'would', 'could', 'should', 'may', 'might', 'must', 'shall',
                    'can', 'need', 'dare', 'ought', 'used', 'to', 'of', 'in',
                    'for', 'on', 'with', 'at', 'by', 'from', 'as', 'into', 
                    'through', 'during', 'before', 'after', 'above', 'below',
                    'between', 'under', 'again', 'further', 'then', 'once'}
        
        filtered = {w: c for w, c in word_count.items() 
                   if w not in stopwords and len(w) > 2}
        
        top_words = sorted(filtered.items(), key=lambda x: -x[1])[:20]
        
        print("\nğŸ“Š æ–‡æœ¬åˆ†æç»“æœ:")
        print(f"   æ€»è¯æ•°: {len(words)}")
        print(f"   é«˜é¢‘è¯ Top 20:")
        for word, count in top_words:
            print(f"   {word}: {count}")
        
        return top_words
    
    def generate_report(self, topic):
        """ç”Ÿæˆç ”ç©¶æŠ¥å‘Š"""
        report = f"""# {topic} ç ”ç©¶æŠ¥å‘Š

ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## æ¦‚è¿°
{topic}æ˜¯ä¸€ä¸ªå€¼å¾—æ·±å…¥ç ”ç©¶çš„é¢†åŸŸã€‚

## ç ”ç©¶æ–¹å‘

### 1. å¸‚åœºç°çŠ¶
ï¼ˆå¾…é‡‡é›†æ•°æ®è¡¥å……ï¼‰

### 2. å‘å±•è¶‹åŠ¿
ï¼ˆå¾…é‡‡é›†æ•°æ®è¡¥å……ï¼‰

### 3. ç«äº‰æ ¼å±€
ï¼ˆå¾…é‡‡é›†æ•°æ®è¡¥å……ï¼‰

## æ•°æ®æ¥æº
- ç½‘ç»œå…¬å¼€æ•°æ®
- è¡Œä¸šæŠ¥å‘Š
- ç¬¬ä¸‰æ–¹ç ”ç©¶

## ç»“è®º
{topic}é¢†åŸŸå…·æœ‰è¾ƒå¤§çš„å‘å±•æ½œåŠ›ï¼Œå»ºè®®æŒç»­å…³æ³¨ã€‚

---
*æœ¬æŠ¥å‘Šç”±AIè‡ªåŠ¨ç”Ÿæˆ*
"""
        
        # ä¿å­˜æŠ¥å‘Š
        filename = f"report_{self.sanitize_filename(topic)}.md"
        filepath = os.path.join(CONFIG['data_dir'], filename)
        
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(report)
        
        print(f"âœ… æŠ¥å‘Šå·²ç”Ÿæˆ: {filename}")
        return report


def main():
    import sys
    
    if len(sys.argv) < 2:
        print("""
æ•°æ®ç ”ç©¶åŠ©æ‰‹ - ä½¿ç”¨è¯´æ˜

ä¾èµ–å®‰è£…:
  pip3 install requests beautifulsoup4 pandas matplotlib

ä½¿ç”¨:
  python3 research_helper.py collect <URL>  # é‡‡é›†ç½‘é¡µ
  python3 research_helper.py list           # åˆ—è¡¨
  python3 research_helper.py analyze <å…³é”®è¯>  # åˆ†æ
  python3 research_helper.py report <ä¸»é¢˜>    # ç”ŸæˆæŠ¥å‘Š

ç¤ºä¾‹:
  python3 research_helper.py collect https://example.com
  python3 research_helper.py report äººå·¥æ™ºèƒ½
""")
        sys.exit(1)
    
    cmd = sys.argv[1]
    helper = ResearchHelper()
    
    if cmd == 'collect' and len(sys.argv) >= 3:
        url = sys.argv[2]
        helper.collect_url(url)
    
    elif cmd == 'list':
        helper.list_collected()
    
    elif cmd == 'analyze' and len(sys.argv) >= 3:
        keyword = ' '.join(sys.argv[2:])
        # ç®€å•æµ‹è¯•åˆ†æ
        helper.analyze_text(keyword * 100)
    
    elif cmd == 'report' and len(sys.argv) >= 3:
        topic = ' '.join(sys.argv[2:])
        helper.generate_report(topic)
    
    else:
        print("å‘½ä»¤é”™è¯¯")


if __name__ == '__main__':
    main()
